{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Extract Shoreline Monitor data (Parquet & CSV), combining series & change-rate\n",
    "\n",
    "## ShorelineMonitor: Satellite-Derived Shoreline-Series\n",
    "\n",
    "The\n",
    "[ShorelineMonitor-Shorelines](https://radiantearth.github.io/stac-browser/#/external/coclico.blob.core.windows.net/stac/v1/shorelinemonitor-shorelines/collection.json)\n",
    "dataset provides Satellite-Derived Shorelines (SDS) extracted from annually composited\n",
    "Landsat satellite imagery spanning the years 1984-2024. These shorelines are mapped onto\n",
    "the [Global Coastal Transect System\n",
    "(GCTS)](https://github.com/TUDelft-CITG/coastpy/blob/main/tutorials/global_coastal_transect_system.ipynb).\n",
    "Together they compose a new dataset that consists of time series per transect.\n",
    "The ShorelineMonitor-Series consists of more than 350 million observations, each with 54\n",
    "attributes, accross almost 7.5 million transects. The dataset and attributes are described in this [STAC\n",
    "collection](https://radiantearth.github.io/stac-browser/#/external/coclico.blob.core.windows.net/stac/v1/shorelinemonitor-series/collection.json).\n",
    "Please have a look at the metadata in one of the items. The dataset is available upon reasonable request. Please contact the data provider for more information or collaboration opportunities.\n",
    "\n",
    "## ShorelineMonitor: Change Rate\n",
    "\n",
    "The ShorelineMonitor dataset provides [Satellite-Derived Shorelines (SDS)](https://radiantearth.github.io/stac-browser/#/external/coclico.blob.core.windows.net/stac/v1/shorelinemonitor-shorelines/collection.json) extracted from annually\n",
    "composited Landsat satellite imagery spanning the years 1984-2024. These shorelines offer a global\n",
    "view of coastal change and shoreline dynamics, serving as a foundation for coastal\n",
    "analytics, modeling and management. The shorelines have been mapped onto the [Global Coastal Transect System](https://radiantearth.github.io/stac-browser/#/external/coclico.blob.core.windows.net/stac/v1/gcts/collection.json) to form a [new dataset](https://radiantearth.github.io/stac-browser/#/external/coclico.blob.core.windows.net/stac/v1/shorelinemonitor-series/collection.json) of more than 7.5 million time-series. \n",
    "\n",
    "This notebook shows how to explore multi-decadal trends in shoreline change that are extracted from the full dataset of time series. The dataset is available upon reasonable request. Please contact the data provider for more information or collaboration opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import pandas as pd\n",
    "import pystac\n",
    "import shapely\n",
    "import geojson\n",
    "import coastpy\n",
    "from dotenv import load_dotenv\n",
    "from ipyleaflet import Map, basemaps, GeoData\n",
    "from shapely.geometry import Polygon, LineString\n",
    "\n",
    "from coastpy.stac.utils import read_snapshot\n",
    "from coastpy.utils.config import fetch_sas_token\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configure cloud and Dask settings\n",
    "sas_token = os.getenv(\"AZURE_STORAGE_SAS_TOKEN\")\n",
    "storage_options = {\"account_name\": \"coclico\", \"sas_token\": sas_token}\n",
    "\n",
    "coclico_catalog = pystac.Catalog.from_file(\n",
    "    \"https://coclico.blob.core.windows.net/stac/v1/catalog.json\"\n",
    ")\n",
    "collection_series = coclico_catalog.get_child(\"shorelinemonitor-series\")\n",
    "collection_change = coclico_catalog.get_child(\"gctr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_fol = r\"p:\\1000545-054-globalbeaches\\04_Shoreline_Monitor_data_requests\\Data_requests\\Bruno_Castelle\"\n",
    "out_fol = r\"p:\\1000545-054-globalbeaches\\04_Shoreline_Monitor_data_requests\\Data_requests\\Bruno_Castelle\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Visualize data chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_series = read_snapshot(collection_series, storage_options=storage_options)\n",
    "#snapshot_series.head()\n",
    "snapshot_series.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_change = read_snapshot(collection_change, storage_options=storage_options)\n",
    "#snapshot_change.head()\n",
    "snapshot_change.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Original data delivery\n",
    "\n",
    "We want to match this approximately, to ensure we are consistent to clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "check = pd.read_csv(os.path.join(out_fol, r\"ShorelineMonitor_1984_2021_v1.5_set1_filtered_extended_sedtypeV2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check.keys())\n",
    "print(check.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Get the AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter based on pre-drawn (in QGIS and exported as .shp and .geojson) AOI\n",
    "\n",
    "# load dataset\n",
    "for files in os.listdir(os.path.join(aoi_fol, 'AOI')):\n",
    "    if 'AOI_FR.geojson' in files:\n",
    "        print(files)\n",
    "        with open(os.path.join(aoi_fol, 'AOI', files)) as f:\n",
    "            gj = geojson.load(f)\n",
    "\n",
    "# revert to polygons\n",
    "polygons = []\n",
    "props = []\n",
    "for i in gj['features']:\n",
    "    try:\n",
    "        polygons.append(Polygon(i['geometry']['coordinates'][0][0]))\n",
    "    except:\n",
    "        polygons.append(Polygon(i['geometry']['coordinates'][0]))\n",
    "    props.append(i['properties']['id'])\n",
    "\n",
    "AOI_gdf = gpd.GeoDataFrame(data={'area': props}, geometry=polygons, crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show it on a map\n",
    "m = Map(basemap=basemaps.Esri.WorldImagery, scroll_wheel_zoom=True)\n",
    "m.center = (0,0)\n",
    "m.zoom = 2\n",
    "m.layout.height = \"900px\"\n",
    "m.add(GeoData(geo_dataframe=AOI_gdf))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of loading an AOI, could also use the map to set a AOI\n",
    "from coastpy.geo.utils import get_region_of_interest_from_map\n",
    "\n",
    "# roi = get_region_of_interest_from_map(m, default_extent=(4.796, 53.108, 5.229, 53.272))\n",
    "# west, south, east, north = roi.geometry.item().bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Fetch data from the databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_change = coastpy.io.STACQueryEngine(\n",
    "    stac_collection=collection_change,\n",
    "    storage_backend=\"azure\",\n",
    "    # columns = [\"geometry\", \"transect_id\", \"sds:change_rate\"] ... # when you don't need all data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_series = coastpy.io.STACQueryEngine(\n",
    "    stac_collection=collection_series,\n",
    "    storage_backend=\"azure\",\n",
    "    # columns = [\"geometry\", \"transect_id\", \"sds:change_rate\"] ... # when you don't need all data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_token = fetch_sas_token(sas_token)\n",
    "df_change = db_change.get_data_within_aoi(AOI_gdf, sas_token=sas_token)\n",
    "print(f\"Shape: {df_change.shape}\")\n",
    "df_change.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_token = fetch_sas_token(sas_token)\n",
    "df_series = db_series.get_data_within_aoi(AOI_gdf, sas_token=sas_token)\n",
    "print(f\"Shape: {df_series.shape}\")\n",
    "df_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make quick checkplot for the df_change\n",
    "m = df_change.explore(color='blue', name='transects')\n",
    "AOI_gdf.explore(m=m, color='red', alpha=0.5, name='AOI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dashboard for the df_series\n",
    "\n",
    "from coastpy.viz.dashboard import ShorelineSeriesApp\n",
    "\n",
    "ShorelineSeriesApp(df_series).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Data alterations for intuitive clean delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns with prob labels\n",
    "df_change_mod = df_change.loc[:, ~df_change.columns.str.contains('prob')]\n",
    "\n",
    "# print some indicators\n",
    "#df_change_mod.keys()\n",
    "df_change_mod.shape\n",
    "#df_change_mod.head(3)\n",
    "\n",
    "# test outcome for one transect\n",
    "# test_change = df_change_mod[df_change_mod.transect_id == \"cl30793s01tr02731365\"]\n",
    "# with pd.option_context('display.max_columns', None):\n",
    "#     display(test_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns from series which are also present in change\n",
    "rem_cols = df_series.columns.intersection(df_change.columns)\n",
    "df_series_mod = df_series.drop(columns=rem_cols[2:]) # do not drop transect_id nor geometry\n",
    "\n",
    "# drop more columns related to the transect\n",
    "df_series_mod = df_series_mod.drop(columns=[\"transect_lon\", \"transect_lat\", \"transect_quadkey\",'tr_stdev', 'tr_range', 'tr_qa_pct', 'tr_is_qa'])\n",
    "\n",
    "# select the primary observations only\n",
    "df_series_mod = df_series_mod[df_series_mod.obs_is_primary == True] \n",
    "\n",
    "# drop the columns that are redundant; because we only select primary observations\n",
    "df_series_mod = df_series_mod.drop(columns=[\"obs_group\", \"obs_is_qa\", \"obs_is_primary\", \"subseries_id\", 'obs_primary_count'])\n",
    "\n",
    "# print some indications\n",
    "#df_series_mod.keys()\n",
    "df_series_mod.shape\n",
    "# df_series_mod.head(3)\n",
    "\n",
    "# test outcome for one transect\n",
    "# test_series = df_series_mod[df_series_mod.transect_id == \"cl30793s01tr02731365\"]\n",
    "# print(test_series.shape)\n",
    "# with pd.option_context('display.max_columns', None):\n",
    "#     display(test_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Write geodataframes to Parquet & CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to parquet\n",
    "df_change_mod.to_parquet(os.path.join(out_fol, r\"SM_FR-transect-rates_1984_2023_set1.parquet\"))\n",
    "\n",
    "# to CSV\n",
    "df_change_mod.to_csv(os.path.join(out_fol, r\"SM_FR-transect-rates_1984_2023_set1.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to parquet\n",
    "df_series_mod.to_parquet(os.path.join(out_fol, r\"SM_FR-time-series_1984_2023_set1.parquet\"))\n",
    "\n",
    "# to CSV\n",
    "df_series_mod.to_csv(os.path.join(out_fol, r\"SM_FR-time-series_1984_2023_set1.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL TODO: merge into one file with nested lists on temporal data for a transect.. ??\n",
    "# TODO: design set1 to 5 for different uses, leaving out certain columns?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coastal-full] *",
   "language": "python",
   "name": "conda-env-coastal-full-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
